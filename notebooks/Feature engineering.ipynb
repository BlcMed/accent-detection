{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Feature Engineering\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_The assertions and methodologies outlined in this notebook are substantiated by referenced scientific studies detailed in the README file._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load libraries and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "from extract_features import *\n",
    "from visualize_audio import *\n",
    "from data_manager import *\n",
    "from preprocess_data import *\n",
    "from load_config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "constants = load_constants_from_yaml('../constants.yml')\n",
    "\n",
    "SAMPLING_RATING = constants[\"SAMPLING_RATING\"]\n",
    "FRAME_LENGTH_ENERGY = constants[\"FRAME_LENGTH_ENERGY\"]\n",
    "THRESHOLD_PERCENTAGE = constants[\"THRESHOLD_PERCENTAGE\"]\n",
    "MIN_SILENCE_DURATION = constants[\"MIN_SILENCE_DURATION\"]\n",
    "HOP_LENGTH = constants[\"HOP_LENGTH\"]\n",
    "SEGMENT_DURATION = constants[\"SEGMENT_DURATION\"]\n",
    "SEGMENT_OVERLAP = constants[\"SEGMENT_OVERLAP\"]\n",
    "N_MFCC = constants[\"N_MFCC\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Segment audio data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# audio_data_segmented = []\n",
    "# labels_segmented = []\n",
    "# for i, audio in enumerate(audio_data_trimmed):\n",
    "#     segments = segment_audio(\n",
    "#         audio, SAMPLING_RATING, duration=SEGMENT_DURATION, overlap=SEGMENT_OVERLAP\n",
    "#     )\n",
    "#     audio_data_segmented.extend(segments)\n",
    "#     labels_segmented.extend([labels_trimmed[i]] * len(segments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"there are {len(audio_data_segmented)} segments .\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extract MFCCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract MFCC features from trimmed audio data (not on segmented audio data because the function itself will split the audio data into segments)\n",
    "mfccs = []\n",
    "for audio in audio_data_trimmed:\n",
    "    mfcc = compute_mfcc(\n",
    "        audio, SAMPLING_RATING, n_mfcc=N_MFCC, duration=SEGMENT_DURATION, overlap=SEGMENT_OVERLAP\n",
    "    )\n",
    "    mfccs.append(mfcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(mfccs): 385\n",
      "num_segments for mfccs[0]: 2318\n",
      "num_segments for mfccs[5]: 2321\n"
     ]
    }
   ],
   "source": [
    "print(f\"len(mfccs): {len(mfccs)}\")\n",
    "print(f\"num_segments for mfccs[0]: {mfccs[0].shape[1]}\")\n",
    "print(f\"num_segments for mfccs[5]: {mfccs[5].shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(segmented_mfccs): 941351\n",
      "len(segmented_labels): 941351\n",
      "segmented_mfccs[0]: 13\n",
      "segmented_labels[0]: english\n"
     ]
    }
   ],
   "source": [
    "# we want to split the mfccs into segments with the corresponding labels\n",
    "\n",
    "# Initialize lists to store segments and their corresponding labels\n",
    "segmented_mfccs = []\n",
    "segmented_labels = []\n",
    "\n",
    "# Iterate over each audio\n",
    "for i, mfcc in enumerate(mfccs):\n",
    "    # Get the shape of MFCCs and label for the current audio\n",
    "    mfcc_shape = mfcc.shape\n",
    "    label = labels_trimmed[i]\n",
    "\n",
    "    # Extract the number of segments and the number of frames per segment\n",
    "    num_segments = mfcc.shape[1]  # Second dimension of the MFCC shape\n",
    "\n",
    "    # Iterate over each segment in the current audio\n",
    "    for j in range(num_segments):\n",
    "        # Extract the MFCCs for the current segment\n",
    "        mfcc_segment = mfccs[i][:, j]\n",
    "\n",
    "        # Append the segment and its corresponding label to the lists\n",
    "        segmented_mfccs.append(mfcc_segment)\n",
    "        segmented_labels.append(label)\n",
    "\n",
    "# Convert the lists to NumPy arrays\n",
    "# segmented_mfccs = np.array(segmented_mfccs)\n",
    "# segmented_labels = np.array(segmented_labels)\n",
    "\n",
    "print(f\"len(segmented_mfccs): {len(segmented_mfccs)}\")\n",
    "print(f\"len(segmented_labels): {len(segmented_labels)}\")\n",
    "print(f\"segmented_mfccs[0]: {len(segmented_mfccs[0])}\")\n",
    "print(f\"segmented_labels[0]: {segmented_labels[0]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
